---
title: "A brief introduction to the GBM algorithm"
author: "James Hickey"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Stochastic gradient boosting is a machine learning algorithm that was developed in two seminal works by J.H. Friedman published in 2001 and 2002.  The process of fitting a boosted model can take on many forms,  with different base models and optimization paradigms.  The purpose of this vignette is to provide a short high-level introduction  to the specific boosting algorithm underlying the GBM package.


## High-level description of stochastic gradient boosting
Gradient boosting generates a low bias predictor by combining lots of high bias weak learners in an additive fashion. More formally, gradient boosting approximates the function $f(\textbf{x})$, which maps covariates to the response variable *y*, as follows:
$$\hat{f}(\textbf{x}) = \hat{f}_{0}(\textbf{x}) + \lambda \sum_{i=1}^{M-1} \hat{f}_{i}(\textbf{x}). \qquad (1)$$
The number of weak learners to fit is denoted by $M$, the initial estimate of the mapping function is $\hat{f}_0$ and the shrinkage parameter is denoted by $\lambda$.  The shrinkage parameter acts as a regularization to prevent overfitting of the model; however, while a smaller shrinkage parameter will tend to prevent overfitting it may require a larger the number of iterations $M$ to achieve a desired performance.  This trade-off is discussed further in the "Getting started with gbm" vignette.

In the GBM package the weak learners are CART decision trees which are fitted to **bagged** observation covariates where the response variable to each covariate is the residual, or error of the current fit, for that observation.  The process of subsampling the training set, known as bagging, and fitting the decision tree to the subsample has the effect of reducing the variance of the fitted model. The residuals to which the trees are fit are calculated via:
$$ z_{i} = -\frac{\partial}{\partial f(\textbf{x}_{\textit{i}})}\Psi(\textit{y}_{\textit{i}},f(\textbf{x}_\textit{i}))|_{f(\textbf{x}_i) = \hat{f}(\textbf{x}_i)},   \qquad (2)$$
where $\Psi(y_i, f(\textbf{x}_i))$ is the loss function (usually the negative of the log likelihood) for the model. This loss function depends on the observation's actual response $y_i$ and the mapping function's predicted response $f(\textbf{x}_i)$ (note in Equation (2) the mapping function is evaluated as the total model estimate at the current boosting iteration).

Now the algorithm uses the shrinkage parameter to reduce the variance of the fitted model. However, another consideration that must be tackled is whether or not the underlying weak learners are individually overfitting to the residuals.  If this is the case the performance of the model on new data will greatly decrease.  The algorithm stops this scenario from occurring by restricting the maximum depth each decision tree can take. 

The idea behind this is that boosted trees are decomposable in terms of "functional ANOVA decomposition". Simply put, the deeper a tree is the higher the order of the interactions between covariates the tree will model, i. e. a "stump" will only model 1st order effects and thus the model will be an additive boosted model.  From this intuition, fixing the maximum depth of a tree to a sensible low order value (generally a depth of $~3-10$ will suffice) reduces the complexity of fitted weak learner and reduces the possibility of overfitting.

Returning to the main algorithm, by fitting decision trees to the residuals the boosted model's training errors are reduced as more and more weak learners are fit, while the threat of overfitting is counteracted via the bagging, shrinkage and truncation of the individual trees. With the tree fit, the optimal terminal node predictions may be calculated via:

$$\rho_k = \underset{\rho}{\text{arg min}} \sum_{\textbf{x}_i \in S_k} \Psi(y_i, \hat{f}(\textbf{x}_i) +\rho),   \qquad (3)$$

where $S_{k}$ is the set of covariate data "in" the terminal node *k* and $\rho_k$ is its optimal prediction.  The optimal predictions then provide the update to the additive sum defined in Equation (1), i. e. $\hat{f}_i(\textbf{x}) = \sum_k \rho_k$.  If the loss associated with a node is independent of the data defining other terminal nodes in the tree, the optimal predictions can be calculated using simple line searching algorithms and sometimes can be analytically derived.  These terminal node predictions determine the predictions made by an individual tree when provided with new covariate data and thus the total boosted model predictions through Equation (1).


To conclude, it should be noted that the residuals in Equation (2) do not depend explicitly on the covariates $\textbf{x}_{i}$.  The residuals capture the error in the current estimate of the response variable and so the only role the covariates play, until Equation (3), is in determining the splits in the tree.  This "degree of freedom" is exploited in the implementation of the Cox proportional hazards model, see the "Guide to the Cox Proportional Hazards model" vignettes for more details.

